{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95e66b01-3893-4379-b315-ede82efeb6d8",
   "metadata": {},
   "source": [
    "## fine tune bert model for custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48530009-5e4d-45aa-875e-4bbb02b9b577",
   "metadata": {},
   "source": [
    "### 1. install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979f468-fe07-4724-9749-2f6af83bc484",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f464fa3a-df71-47e2-b44c-12570f0375f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b538518-a536-4fda-b99f-0af0929f7294",
   "metadata": {},
   "source": [
    "### 2. load/define data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f665b5-5346-4a77-9ec5-dc4813f2d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "\n",
    "def applylimit(df: pd.DataFrame, n: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param df: data frame with 'label' column\n",
    "    :param n: min number of samples required\n",
    "    :return: data frame with minimum number of samples per label\n",
    "    \"\"\"\n",
    "    dfcounts = df.groupby('label').size().reset_index(name='counts')\n",
    "    dfmincounts = dfcounts[dfcounts.counts > n]\n",
    "    return df[df.label.isin(dfmincounts.label.tolist())]\n",
    "\n",
    "\n",
    "def encodelabels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param df: data frame with 'label' column\n",
    "    :return: data frame with 'intlabel' column containing encoded labels\n",
    "    \"\"\"\n",
    "    df['intlabel'] = df['label'].rank(method='dense', ascending=False).astype(int) - 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def loadpreprocesseddata(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param path: absolute path to a csv file with 'label' column\n",
    "    :return: data frame with labels mapped to\n",
    "        integer values in the intlabel column\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    df = applylimit(df=df, n=2)\n",
    "    df = encodelabels(df=df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def getmapping(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    :param data: data frame with 'intlabel' columns containing int values\n",
    "        and 'label' column containing str values\n",
    "    :return: mapping from int values to str\n",
    "    \"\"\"\n",
    "    labelmapping = {}\n",
    "    for key in data.intlabel.unique():\n",
    "        value = data.loc[data['intlabel'] == key, 'label'].unique()[0]\n",
    "        labelmapping[key] = value\n",
    "    return labelmapping\n",
    "\n",
    "\n",
    "def splitdata(data: pd.DataFrame, ratio: float = 0.2) -> dict:\n",
    "    \"\"\"\n",
    "    :param data: data frame with 'text' and 'intlabel' columns\n",
    "    :param ratio: ratio of a test set to a data set\n",
    "    :return: train and test data sets\n",
    "    \"\"\"\n",
    "    texts = data.text.tolist()\n",
    "    labels = data.intlabel.tolist()\n",
    "    trntxt, tsttxt, trnlbl, tstlbl = train_test_split(texts, labels, test_size=ratio)\n",
    "    return {\"train\": {\"text\": trntxt, \"label\": trnlbl},\n",
    "            \"test\": {\"text\": tsttxt, \"label\": tstlbl}}\n",
    "\n",
    "\n",
    "def balancedata(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    :param data: dictionary with 'text' and 'label' keys\n",
    "    :return: balanced dataset\n",
    "    \"\"\"\n",
    "    sampler = RandomOverSampler(random_state=42)\n",
    "    txt = np.asarray(data[\"text\"])\n",
    "    txt = txt[:, np.newaxis]\n",
    "    txt, lbl = sampler.fit_resample(txt, data[\"label\"])\n",
    "    txt = txt.flatten().tolist()\n",
    "    return {\"text\": txt, \"label\": lbl}\n",
    "\n",
    "\n",
    "def encodefeatures(data: dict, tokenizer) -> list:\n",
    "    \"\"\"\n",
    "    :param data: dictionary with 'text' and 'label' keys\n",
    "    :param tokenizer: encode text into vectors with integer values\n",
    "    :return: list of dicts with encoded data\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(data[\"text\"], truncation=True, padding=True)\n",
    "    zipped = zip(data[\"label\"], encodings['input_ids'], encodings['attention_mask'])\n",
    "    return [{'label': label,\n",
    "             'input_ids': input_id,\n",
    "             'attention_mask': attention_mask} for label, input_id, attention_mask in zipped]\n",
    "\n",
    "\n",
    "def countlabels(data: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    :param data: dictionary with a 'label' key and one feature key\n",
    "    :return: data frame with 'counts' column containing\n",
    "        number of samples per label\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    return df.groupby('label').size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, path: str, tokenizer):\n",
    "        \"\"\"\n",
    "        :param path: relative to a data folder path to a csv file with two columns 'text', 'label'\n",
    "        :param tokenizer: encode text into vectors with integer values.\n",
    "            loaded with from_pretrained() function for a model that is about to be tuned\n",
    "        \"\"\"\n",
    "        self.data = loadpreprocesseddata(path=f\"{DATA_DIR}/{path}\")\n",
    "        self.labelmapping = getmapping(data=self.data)\n",
    "        self.nlabels = len(self.labelmapping.values())\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datasets = {}\n",
    "        self.trainset = []\n",
    "        self.testset = []\n",
    "        self.resamplesets()\n",
    "\n",
    "    def reloaddata(self, path: str):\n",
    "        \"\"\"\n",
    "        Reload and preprocess again raw data\n",
    "\n",
    "        :param path: relative to a data folder path to a csv file with two columns 'text', 'label'\n",
    "        \"\"\"\n",
    "        self.data = loadpreprocesseddata(path=f\"{DATA_DIR}/{path}\")\n",
    "        self.labelmapping = getmapping(data=self.data)\n",
    "        self.nlabels = len(self.labelmapping.values())\n",
    "        self.resamplesets()\n",
    "\n",
    "    def resamplesets(self):\n",
    "        \"\"\"\n",
    "        Randomly split data into train and test sets\n",
    "        \"\"\"\n",
    "        self.datasets = splitdata(data=self.data)\n",
    "        self.datasets[\"train\"] = balancedata(data=self.datasets[\"train\"])\n",
    "        self.trainset = encodefeatures(data=self.datasets[\"train\"], tokenizer=self.tokenizer)\n",
    "        self.testset = encodefeatures(data=self.datasets[\"test\"], tokenizer=self.tokenizer)\n",
    "\n",
    "    def getdistribution(self, name: str):\n",
    "        \"\"\"\n",
    "        :param name: name of a subset: train/test\n",
    "        :return: data frame containing number of\n",
    "            samples per label in a train dataset\n",
    "        \"\"\"\n",
    "        return countlabels(data=self.datasets[name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ce3fc-994a-4abe-a0b2-bfd9e2ad1516",
   "metadata": {},
   "source": [
    "### 4. load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7027aa-917e-4237-849e-48c0cee7e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification as AMSC\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "CONFIG_DIR = \"./config\"\n",
    "\n",
    "\n",
    "def showlearningcurve(loss: list, evalloss: list):\n",
    "    \"\"\"\n",
    "    :param loss: list of train loss values\n",
    "    :param evalloss: list of evaluation loss values\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    epochs = np.arange(len(loss))\n",
    "    plt.plot(epochs, loss, color='b')\n",
    "    plt.plot(epochs, evalloss, color='r')\n",
    "    plt.legend(['train loss', 'test loss'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class FineTuner:\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        :param config: configuration with training parameters.\n",
    "            required are: 'modelname', 'datapath'\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"modelname\"])\n",
    "        self.dm = DataManager(path=self.config[\"datapath\"], tokenizer=self.tokenizer)\n",
    "        self.model = AMSC.from_pretrained(self.config[\"modelname\"], num_labels=self.dm.nlabels)\n",
    "        self.args = self.getargs()\n",
    "        self.trainer = self.gettrainer()\n",
    "\n",
    "    def reloaddata(self):\n",
    "        \"\"\"\n",
    "        Reload and preprocess raw data\n",
    "        \"\"\"\n",
    "        self.dm.reloaddata(path=self.config[\"datapath\"])\n",
    "\n",
    "    def resample(self):\n",
    "        \"\"\"\n",
    "        Randomly resample train and test sets\n",
    "        \"\"\"\n",
    "        self.dm.resamplesets()\n",
    "\n",
    "    def reloadmodel(self):\n",
    "        \"\"\"\n",
    "        Reload model for fine tuning\n",
    "        \"\"\"\n",
    "        self.model = AMSC.from_pretrained(self.config[\"modelname\"], num_labels=self.dm.nlabels)\n",
    "\n",
    "    def getargs(self) -> TrainingArguments:\n",
    "        \"\"\"\n",
    "        :return: configured training arguments\n",
    "        \"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=\"./tunedbert\",\n",
    "            do_eval=True,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=self.config[\"lr\"],\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            logging_strategy=\"epoch\",\n",
    "            num_train_epochs=self.config[\"nepochs\"],\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "\n",
    "    def gettrainer(self) -> Trainer:\n",
    "        \"\"\"\n",
    "        :return: configured trainer\n",
    "        \"\"\"\n",
    "        return Trainer(\n",
    "            model=self.model,\n",
    "            args=self.args,\n",
    "            train_dataset=self.dm.trainset,\n",
    "            eval_dataset=self.dm.testset,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "    def train(self, learningcurve: bool = False) -> (list, list):\n",
    "        \"\"\"\n",
    "        :param learningcurve: if true show learning curve after training\n",
    "        :return: (training loss, evaluation loss)\n",
    "        \"\"\"\n",
    "        self.trainer.train()\n",
    "        history = np.asarray(self.trainer.state.log_history[:-1])\n",
    "        loss = [entry['loss'] for entry in history[::2]]\n",
    "        evalloss = [entry['eval_loss'] for entry in history[1::2]]\n",
    "        if learningcurve:\n",
    "            showlearningcurve(loss=loss, evalloss=evalloss)\n",
    "        return loss, evalloss\n",
    "\n",
    "    def predictbatch(self, batch: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param batch: list of encoded inputs\n",
    "        :return: numpy array of predicted labels\n",
    "        \"\"\"\n",
    "        predictions = self.trainer.predict(batch)\n",
    "        return np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "    def humanpredict(self, sentence: str) -> str:\n",
    "        \"\"\"\n",
    "        :return: predicted label\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a904b-61d2-4b5b-9c59-5d1f48801adf",
   "metadata": {},
   "source": [
    "### 6. Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91eb97ec-4051-4b27-a889-268cb9dc4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import sys\n",
    "\n",
    "\n",
    "def loadconfig(path: str) -> dict:\n",
    "    \"\"\"\n",
    "    :param path: path to a configuration file\n",
    "    :return: configurations as a dictionary\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        try:\n",
    "            return yaml.load(stream=f, Loader=yaml.FullLoader)\n",
    "        except IOError as e:\n",
    "            sys.exit(f\"FAILED TO LOAD CONFIG {path}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.trials = []\n",
    "        config = loadconfig(path=f\"{CONFIG_DIR}/finetune.yaml\")\n",
    "        self.tuner = FineTuner(config=config)\n",
    "\n",
    "    def evaluate(self) -> dict:\n",
    "        \"\"\"\n",
    "        Train and evaluate fine tuned model\n",
    "        :return: dictionary with results\n",
    "        \"\"\"\n",
    "        self.tuner.train()\n",
    "        batch = self.tuner.dm.testset\n",
    "        predictions = self.tuner.predictbatch(batch=batch)\n",
    "        groundtruth = np.array([entry['label'] for entry in batch])\n",
    "        correct = np.sum(predictions == groundtruth)\n",
    "        accuracy = correct / groundtruth.shape[0]\n",
    "        return {\"accuracy\": accuracy,\n",
    "                \"correct\": correct,\n",
    "                \"total\": groundtruth.shape[0],\n",
    "                \"predicted\": predictions,\n",
    "                \"groundtruth\": groundtruth}\n",
    "\n",
    "    def processresults(self) -> dict:\n",
    "        \"\"\"\n",
    "        :return: mean and total results respective to the metrics for all trials combined\n",
    "        \"\"\"\n",
    "        results = {\"accuracy\": 0, \"correct\": 0, \"total\": 0}\n",
    "        for trial in self.trials:\n",
    "            for key in results.keys():\n",
    "                results[key] += trial[key]\n",
    "        results[\"accuracy\"] /= len(self.trials)\n",
    "        return results\n",
    "\n",
    "    def runevaluation(self, n: int = 5) -> dict:\n",
    "        \"\"\"\n",
    "        Run evaluations n times and return mean score\n",
    "\n",
    "        :param n: number of evaluation iterations\n",
    "        :return: dictionary with results\n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "            self.trials.append(self.evaluate())\n",
    "            print(f\"TRIAL {i}; ACCURACY: {self.trials[-1]['accuracy']}\")\n",
    "            self.tuner.resample()\n",
    "            self.tuner.reloadmodel()\n",
    "        return self.processresults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c5ad6-6bd4-4621-841c-212215b2d94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
