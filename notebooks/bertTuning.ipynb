{"cells":[{"cell_type":"markdown","id":"95e66b01-3893-4379-b315-ede82efeb6d8","metadata":{"id":"95e66b01-3893-4379-b315-ede82efeb6d8"},"source":["## fine tune bert model for custom dataset"]},{"cell_type":"markdown","id":"48530009-5e4d-45aa-875e-4bbb02b9b577","metadata":{"id":"48530009-5e4d-45aa-875e-4bbb02b9b577"},"source":["### 1. install libraries"]},{"cell_type":"code","execution_count":1,"id":"1979f468-fe07-4724-9749-2f6af83bc484","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1979f468-fe07-4724-9749-2f6af83bc484","executionInfo":{"status":"ok","timestamp":1648629655737,"user_tz":-120,"elapsed":21509,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}},"outputId":"a3519591-7afe-48a6-bbee-1b4b7bf7671f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 5.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 23.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 34.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 31.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.17.0\n"]}],"source":["! pip install transformers"]},{"cell_type":"code","execution_count":2,"id":"f464fa3a-df71-47e2-b44c-12570f0375f6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f464fa3a-df71-47e2-b44c-12570f0375f6","executionInfo":{"status":"ok","timestamp":1648629661724,"user_tz":-120,"elapsed":6010,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}},"outputId":"7a745661-5cd7-432e-cc2f-f0293be49818"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.1.0)\n"]}],"source":["! pip install imbalanced-learn"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIoZILop-2xd","executionInfo":{"status":"ok","timestamp":1648629682381,"user_tz":-120,"elapsed":20671,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}},"outputId":"49dc37f3-58f4-4750-961b-40c272e93031"},"id":"lIoZILop-2xd","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"5b538518-a536-4fda-b99f-0af0929f7294","metadata":{"id":"5b538518-a536-4fda-b99f-0af0929f7294"},"source":["### 2. load/define data set"]},{"cell_type":"code","execution_count":6,"id":"78f665b5-5346-4a77-9ec5-dc4813f2d720","metadata":{"id":"78f665b5-5346-4a77-9ec5-dc4813f2d720","executionInfo":{"status":"ok","timestamp":1648629716301,"user_tz":-120,"elapsed":896,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}}},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import RandomOverSampler\n","from transformers import AutoTokenizer\n","import numpy as np\n","DATA_DIR = \"drive/MyDrive/data\"\n","\n","\n","def applylimit(df: pd.DataFrame, n: int = 2) -> pd.DataFrame:\n","    \"\"\"\n","    :param df: data frame with 'label' column\n","    :param n: min number of samples required\n","    :return: data frame with minimum number of samples per label\n","    \"\"\"\n","    dfcounts = df.groupby('label').size().reset_index(name='counts')\n","    dfmincounts = dfcounts[dfcounts.counts > n]\n","    return df[df.label.isin(dfmincounts.label.tolist())]\n","\n","\n","def encodelabels(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    :param df: data frame with 'label' column\n","    :return: data frame with 'intlabel' column containing encoded labels\n","    \"\"\"\n","    df['intlabel'] = df['label'].rank(method='dense', ascending=False).astype(int) - 1\n","    return df\n","\n","\n","def loadpreprocesseddata(path: str) -> pd.DataFrame:\n","    \"\"\"\n","    :param path: absolute path to a csv file with 'label' column\n","    :return: data frame with labels mapped to\n","        integer values in the intlabel column\n","    \"\"\"\n","    df = pd.read_csv(path)\n","    df = applylimit(df=df, n=2)\n","    df = encodelabels(df=df)\n","    return df\n","\n","\n","def getmapping(data: pd.DataFrame) -> dict:\n","    \"\"\"\n","    :param data: data frame with 'intlabel' columns containing int values\n","        and 'label' column containing str values\n","    :return: mapping from int values to str\n","    \"\"\"\n","    labelmapping = {}\n","    for key in data.intlabel.unique():\n","        value = data.loc[data['intlabel'] == key, 'label'].unique()[0]\n","        labelmapping[key] = value\n","    return labelmapping\n","\n","\n","def splitdata(data: pd.DataFrame, ratio: float = 0.2) -> dict:\n","    \"\"\"\n","    :param data: data frame with 'text' and 'intlabel' columns\n","    :param ratio: ratio of a test set to a data set\n","    :return: train and test data sets\n","    \"\"\"\n","    texts = data.text.tolist()\n","    labels = data.intlabel.tolist()\n","    trntxt, tsttxt, trnlbl, tstlbl = train_test_split(texts, labels, test_size=ratio)\n","    return {\"train\": {\"text\": trntxt, \"label\": trnlbl},\n","            \"test\": {\"text\": tsttxt, \"label\": tstlbl}}\n","\n","\n","def balancedata(data: dict) -> dict:\n","    \"\"\"\n","    :param data: dictionary with 'text' and 'label' keys\n","    :return: balanced dataset\n","    \"\"\"\n","    sampler = RandomOverSampler(random_state=42)\n","    txt = np.asarray(data[\"text\"])\n","    txt = txt[:, np.newaxis]\n","    txt, lbl = sampler.fit_resample(txt, data[\"label\"])\n","    txt = txt.flatten().tolist()\n","    return {\"text\": txt, \"label\": lbl}\n","\n","\n","def encodefeatures(data: dict, tokenizer) -> list:\n","    \"\"\"\n","    :param data: dictionary with 'text' and 'label' keys\n","    :param tokenizer: encode text into vectors with integer values\n","    :return: list of dicts with encoded data\n","    \"\"\"\n","    encodings = tokenizer(data[\"text\"], truncation=True, padding=True)\n","    zipped = zip(data[\"label\"], encodings['input_ids'], encodings['attention_mask'])\n","    return [{'label': label,\n","             'input_ids': input_id,\n","             'attention_mask': attention_mask} for label, input_id, attention_mask in zipped]\n","\n","\n","def countlabels(data: dict) -> pd.DataFrame:\n","    \"\"\"\n","    :param data: dictionary with a 'label' key and one feature key\n","    :return: data frame with 'counts' column containing\n","        number of samples per label\n","    \"\"\"\n","    df = pd.DataFrame(data)\n","    return df.groupby('label').size().reset_index(name='counts')\n","\n","\n","class DataManager:\n","    def __init__(self, path: str, tokenizer):\n","        \"\"\"\n","        :param path: relative to a data folder path to a csv file with two columns 'text', 'label'\n","        :param tokenizer: encode text into vectors with integer values.\n","            loaded with from_pretrained() function for a model that is about to be tuned\n","        \"\"\"\n","        self.data = loadpreprocesseddata(path=f\"{DATA_DIR}/{path}\")\n","        self.labelmapping = getmapping(data=self.data)\n","        self.nlabels = len(self.labelmapping.values())\n","        self.tokenizer = tokenizer\n","        self.datasets = {}\n","        self.trainset = []\n","        self.testset = []\n","        self.resamplesets()\n","\n","    def reloaddata(self, path: str):\n","        \"\"\"\n","        Reload and preprocess again raw data\n","\n","        :param path: relative to a data folder path to a csv file with two columns 'text', 'label'\n","        \"\"\"\n","        self.data = loadpreprocesseddata(path=f\"{DATA_DIR}/{path}\")\n","        self.labelmapping = getmapping(data=self.data)\n","        self.nlabels = len(self.labelmapping.values())\n","        self.resamplesets()\n","\n","    def resamplesets(self):\n","        \"\"\"\n","        Randomly split data into train and test sets\n","        \"\"\"\n","        self.datasets = splitdata(data=self.data)\n","        self.datasets[\"train\"] = balancedata(data=self.datasets[\"train\"])\n","        self.trainset = encodefeatures(data=self.datasets[\"train\"], tokenizer=self.tokenizer)\n","        self.testset = encodefeatures(data=self.datasets[\"test\"], tokenizer=self.tokenizer)\n","\n","    def getdistribution(self, name: str):\n","        \"\"\"\n","        :param name: name of a subset: train/test\n","        :return: data frame containing number of\n","            samples per label in a train dataset\n","        \"\"\"\n","        return countlabels(data=self.datasets[name])\n"]},{"cell_type":"markdown","id":"b02ce3fc-994a-4abe-a0b2-bfd9e2ad1516","metadata":{"id":"b02ce3fc-994a-4abe-a0b2-bfd9e2ad1516"},"source":["### 4. load pretrained model"]},{"cell_type":"code","execution_count":7,"id":"af7027aa-917e-4237-849e-48c0cee7e0a6","metadata":{"id":"af7027aa-917e-4237-849e-48c0cee7e0a6","executionInfo":{"status":"ok","timestamp":1648629725964,"user_tz":-120,"elapsed":236,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}}},"outputs":[],"source":["from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification as AMSC\n","from transformers import TrainingArguments, Trainer\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","CONFIG_DIR = \"drive/MyDrive/configs\"\n","\n","\n","def showlearningcurve(loss: list, evalloss: list):\n","    \"\"\"\n","    :param loss: list of train loss values\n","    :param evalloss: list of evaluation loss values\n","    \"\"\"\n","    plt.figure()\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    epochs = np.arange(len(loss))\n","    plt.plot(epochs, loss, color='b')\n","    plt.plot(epochs, evalloss, color='r')\n","    plt.legend(['train loss', 'test loss'])\n","    plt.show()\n","\n","\n","class FineTuner:\n","    def __init__(self, config: dict):\n","        \"\"\"\n","        :param config: configuration with training parameters.\n","            required are: 'modelname', 'datapath'\n","        \"\"\"\n","        self.config = config\n","        self.tokenizer = AutoTokenizer.from_pretrained(self.config[\"modelname\"])\n","        self.dm = DataManager(path=self.config[\"datapath\"], tokenizer=self.tokenizer)\n","        self.model = AMSC.from_pretrained(self.config[\"modelname\"], num_labels=self.dm.nlabels)\n","        self.args = self.getargs()\n","        self.trainer = self.gettrainer()\n","\n","    def reloaddata(self):\n","        \"\"\"\n","        Reload and preprocess raw data\n","        \"\"\"\n","        self.dm.reloaddata(path=self.config[\"datapath\"])\n","\n","    def resample(self):\n","        \"\"\"\n","        Randomly resample train and test sets\n","        \"\"\"\n","        self.dm.resamplesets()\n","\n","    def reloadmodel(self):\n","        \"\"\"\n","        Reload model for fine tuning\n","        \"\"\"\n","        self.model = AMSC.from_pretrained(self.config[\"modelname\"], num_labels=self.dm.nlabels)\n","\n","    def getargs(self) -> TrainingArguments:\n","        \"\"\"\n","        :return: configured training arguments\n","        \"\"\"\n","        return TrainingArguments(\n","            output_dir=\"./tunedbert\",\n","            do_eval=True,\n","            evaluation_strategy=\"epoch\",\n","            learning_rate=self.config[\"lr\"],\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            logging_strategy=\"epoch\",\n","            num_train_epochs=self.config[\"nepochs\"],\n","            weight_decay=0.01,\n","        )\n","\n","    def gettrainer(self) -> Trainer:\n","        \"\"\"\n","        :return: configured trainer\n","        \"\"\"\n","        return Trainer(\n","            model=self.model,\n","            args=self.args,\n","            train_dataset=self.dm.trainset,\n","            eval_dataset=self.dm.testset,\n","            tokenizer=self.tokenizer,\n","        )\n","\n","    def train(self, learningcurve: bool = False) -> (list, list):\n","        \"\"\"\n","        :param learningcurve: if true show learning curve after training\n","        :return: (training loss, evaluation loss)\n","        \"\"\"\n","        self.trainer.train()\n","        history = np.asarray(self.trainer.state.log_history[:-1])\n","        loss = [entry['loss'] for entry in history[::2]]\n","        evalloss = [entry['eval_loss'] for entry in history[1::2]]\n","        if learningcurve:\n","            showlearningcurve(loss=loss, evalloss=evalloss)\n","        return loss, evalloss\n","\n","    def predictbatch(self, batch: list) -> np.ndarray:\n","        \"\"\"\n","        :param batch: list of encoded inputs\n","        :return: numpy array of predicted labels\n","        \"\"\"\n","        predictions = self.trainer.predict(batch)\n","        return np.argmax(predictions.predictions, axis=1)\n","\n","    def humanpredict(self, sentence: str) -> str:\n","        \"\"\"\n","        :return: predicted label\n","        \"\"\"\n","        pass\n"]},{"cell_type":"markdown","id":"e95a904b-61d2-4b5b-9c59-5d1f48801adf","metadata":{"id":"e95a904b-61d2-4b5b-9c59-5d1f48801adf"},"source":["### 6. Test trained model"]},{"cell_type":"code","execution_count":8,"id":"91eb97ec-4051-4b27-a889-268cb9dc4475","metadata":{"id":"91eb97ec-4051-4b27-a889-268cb9dc4475","executionInfo":{"status":"ok","timestamp":1648629728475,"user_tz":-120,"elapsed":494,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}}},"outputs":[],"source":["import numpy as np\n","import yaml\n","import sys\n","\n","def loadconfig(path: str) -> dict:\n","    \"\"\"\n","    :param path: path to a configuration file\n","    :return: configurations as a dictionary\n","    \"\"\"\n","    with open(path) as f:\n","        try:\n","            return yaml.load(stream=f, Loader=yaml.FullLoader)\n","        except IOError as e:\n","            sys.exit(f\"FAILED TO LOAD CONFIG {path}: {e}\")\n","\n","\n","\n","class Evaluator:\n","    def __init__(self):\n","        self.trials = []\n","        config = loadconfig(path=f\"{CONFIG_DIR}/finetune.yaml\")\n","        self.tuner = FineTuner(config=config)\n","\n","    def evaluate(self) -> dict:\n","        \"\"\"\n","        Train and evaluate fine tuned model\n","        :return: dictionary with results\n","        \"\"\"\n","        self.tuner.train()\n","        batch = self.tuner.dm.testset\n","        predictions = self.tuner.predictbatch(batch=batch)\n","        groundtruth = np.array([entry['label'] for entry in batch])\n","        correct = np.sum(predictions == groundtruth)\n","        accuracy = correct / groundtruth.shape[0]\n","        return {\"accuracy\": accuracy,\n","                \"correct\": correct,\n","                \"total\": groundtruth.shape[0],\n","                \"predicted\": predictions,\n","                \"groundtruth\": groundtruth}\n","\n","    def processresults(self) -> dict:\n","        \"\"\"\n","        :return: mean and total results respective to the metrics for all trials combined\n","        \"\"\"\n","        results = {\"accuracy\": 0, \"correct\": 0, \"total\": 0}\n","        for trial in self.trials:\n","            for key in results.keys():\n","                results[key] += trial[key]\n","        results[\"accuracy\"] /= len(self.trials)\n","        return results\n","\n","    def runevaluation(self, n: int = 5) -> dict:\n","        \"\"\"\n","        Run evaluations n times and return mean score\n","\n","        :param n: number of evaluation iterations\n","        :return: dictionary with results\n","        \"\"\"\n","        for i in range(n):\n","            self.trials.append(self.evaluate())\n","            print(f\"TRIAL {i}; ACCURACY: {self.trials[-1]['accuracy']}\")\n","            self.tuner.resample()\n","            self.tuner.reloadmodel()\n","        return self.processresults()"]},{"cell_type":"code","execution_count":10,"id":"049c5ad6-6bd4-4621-841c-212215b2d94f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a145b426a76e4d60afd3f47604e886c8","2a9069808874423eafabc8cf0fd01129","052a41705c6848f89921aa1754ec2cd8","6654138f106847b88af7e85998ac6212","889ae363ce93458796bc404a56ac2055","5f1089849a294cb5adfa27c14d6c6cd7","cffd692b7be448328cf21189d0bf0a92","4d02728df92b47f9877826945c724dfa","f434ded0230542b3a763d8e9252633ed","6090d4ff448e4e29aa810bf34f6c1678","6b1ce65d10b9475cbb76ee808465cca9","650763fdff364a9ba49c8df6de752df4","ad16009ecd744b4793bc3ad5c1c4070c","39ee340158464be4abcdd88b8a3ae922","2e2ae3b359e6479581108a8a831ea3b1","799b59610d804f2db660a50bd56734c0","be753dc64c0940968a19dfa50e973ece","b4a305349c1244c99f5cff87497c98c4","c0010d7b0b4d41caad8cd64b61917158","a6769b62f9194f39ab18881a606aae52","1bea93ddb82d4fe6a9b381677425451c","ebb9d50eb23047b180769c9f94bd9de4","7ffce6cbe1be4b7097ba044644909e21","77e5b8cfaf48476b8faf0e41ae5c8527","d2860526b9c0406eb226ce9fc55affe6","89e1dc3981504c6099654dae542cd8f1","428a1b76726d487da85c1f5da374e230","800007ce6c0240428052863a69eb556c","2888f3e7fbb441079d526045fa9d90e4","ca1688a5b98d4e8e8edc5d2d4b5013b9","d4ca904cb3fa4963a148f02d7ac3f2b7","c83e3a307038482a9b9a519b3746637c","57431ebcd1c74e41acae04c81069ff4c","3cd78538731d439887d49ecf62bf1983","66b0e20fd801478dba87365e1cfc21ed","8f038fa32d1749b2b09086992b95db0c","ef0776a9fc864f42874b72af8262b92d","3af79938ceed44a9a93dfbc15b537d39","2772b9d2524a41c4b5cba7e581bc7796","3e7652099b4d4039a418151202da6815","5ce328c7084e4e709defaafb0f9f3602","93822d4deae64a87925352c8a8e3d4f2","de6bb4b1ed84496d953a23edf228759d","895bdca74aa04404b9e55538e9a987d0","e1d50d684c4648ad84a6ee44ea7ae76f","cbafaca1b32049efb814020f8b1f0e71","f62d5157a315412ba7a3c2e163aa4a25","89988da09b5a4db7a6a5762e3737d926","7bbac0199ab84c198eabd88b1f38691a","47cad33315c247a0a30e2814dce64b19","aa147de9b13c4afbb9d9f38edaf65503","51693424611146738c81ffc70c904ca0","595c24a6be1e47dab851407940d3d474","3ed6e03f1ab94beba1cf1af576b4b47a","0d0b2a78e4704514ae387c5bcc49225b","523ff370fef84e698441a8c3f75cea27","0815bd3c9f7a45838d4efd9cb63115f5","f8505269a06444f7b6f01475b79a9527","c50c7a4210f8451ca2dfb077745d424e","8d690302588d4d73ad6d5d2e1db5d91c","c246930798614bce86a0bdd6406c2249","6de1063007034effa5640ac03f9f3e0f","b1c19baba75b4c7fb2065f7f7b7329f6","126d3e4e56b640a9ab43c8a0aa020bd1","a677dce821544515a84844d9b868371e","3ab8819f5a0049dc9ff76b165734ef19"]},"id":"049c5ad6-6bd4-4621-841c-212215b2d94f","executionInfo":{"status":"ok","timestamp":1648630043706,"user_tz":-120,"elapsed":192092,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}},"outputId":"31eefdd7-4d40-470e-ee24-4874e9dec5ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi45p0n6s\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/402 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a145b426a76e4d60afd3f47604e886c8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ab333f0c0e8ca2b891af2b3aeeea5bb0a6817083df22b6fb248a065904a7e032.199f68f9cbacfa8c1ea1f23db0b30d113d1d1ec00c9041f48cd39ab65741338a\n","creating metadata file for /root/.cache/huggingface/transformers/ab333f0c0e8ca2b891af2b3aeeea5bb0a6817083df22b6fb248a065904a7e032.199f68f9cbacfa8c1ea1f23db0b30d113d1d1ec00c9041f48cd39ab65741338a\n","https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp86_ds5um\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/723 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650763fdff364a9ba49c8df6de752df4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","creating metadata file for /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqiuhctki\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffce6cbe1be4b7097ba044644909e21"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/sentencepiece.bpe.model in cache at /root/.cache/huggingface/transformers/3477950aabc6f988b3f5c58ea0a1996dcf8f5f2c12647011f120b3be1c8ee90a.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","creating metadata file for /root/.cache/huggingface/transformers/3477950aabc6f988b3f5c58ea0a1996dcf8f5f2c12647011f120b3be1c8ee90a.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2b3afpbd\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd78538731d439887d49ecf62bf1983"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/b26c00523dd0cefb9eca356096c6771639f599068dbd4cdeb56a1b9698211208.c405a8c40385cf4ccadaf968f41d7b7a1a4e35124e51455cffa458d1f8d2c552\n","creating metadata file for /root/.cache/huggingface/transformers/b26c00523dd0cefb9eca356096c6771639f599068dbd4cdeb56a1b9698211208.c405a8c40385cf4ccadaf968f41d7b7a1a4e35124e51455cffa458d1f8d2c552\n","https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwsbvugmb\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d50d684c4648ad84a6ee44ea7ae76f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/4ac3a98f3bb4ac724ac3f0ad472e1955687f94bfa55a8b907fe23549b27429b4.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n","creating metadata file for /root/.cache/huggingface/transformers/4ac3a98f3bb4ac724ac3f0ad472e1955687f94bfa55a8b907fe23549b27429b4.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n","loading file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/3477950aabc6f988b3f5c58ea0a1996dcf8f5f2c12647011f120b3be1c8ee90a.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","loading file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b26c00523dd0cefb9eca356096c6771639f599068dbd4cdeb56a1b9698211208.c405a8c40385cf4ccadaf968f41d7b7a1a4e35124e51455cffa458d1f8d2c552\n","loading file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/4ac3a98f3bb4ac724ac3f0ad472e1955687f94bfa55a8b907fe23549b27429b4.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n","loading file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ab333f0c0e8ca2b891af2b3aeeea5bb0a6817083df22b6fb248a065904a7e032.199f68f9cbacfa8c1ea1f23db0b30d113d1d1ec00c9041f48cd39ab65741338a\n","loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf4wjacle\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523ff370fef84e698441a8c3f75cea27"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","creating metadata file for /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","loading weights file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","Some weights of the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 were not used when initializing XLMRobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 368\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 69\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [69/69 00:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.509100</td>\n","      <td>1.979231</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.278000</td>\n","      <td>1.299065</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.764700</td>\n","      <td>1.117013</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 33\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["TRIAL 0; ACCURACY: 0.7878787878787878\n"]},{"output_type":"stream","name":"stderr","text":["loading weights file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","Some weights of the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 were not used when initializing XLMRobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 368\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 69\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [69/69 00:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.675800</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.672800</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.673800</td>\n","      <td>1.117013</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 33\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["TRIAL 1; ACCURACY: 0.9696969696969697\n"]},{"output_type":"stream","name":"stderr","text":["loading weights file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","Some weights of the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 were not used when initializing XLMRobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 368\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 69\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [69/69 00:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.675400</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.676200</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.669900</td>\n","      <td>1.117013</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 33\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["TRIAL 2; ACCURACY: 0.9393939393939394\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","Some weights of the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 were not used when initializing XLMRobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 368\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 69\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [69/69 00:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.671100</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.671100</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.675900</td>\n","      <td>1.117013</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 33\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["TRIAL 3; ACCURACY: 0.9393939393939394\n"]},{"output_type":"stream","name":"stderr","text":["loading weights file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","Some weights of the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 were not used when initializing XLMRobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","***** Running training *****\n","  Num examples = 368\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 69\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [69/69 00:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.668200</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.670200</td>\n","      <td>1.117013</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.671000</td>\n","      <td>1.117013</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","***** Running Evaluation *****\n","  Num examples = 33\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 33\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ded7802c9a1fa89eb9dd9f457a200398369bbf5210a16b33996f5068c73e3a15.ba455ec869fd0d70e7e3b2b1fd62bf88c960c377df590baf894f0ab00d113f2c\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]},{"output_type":"stream","name":"stdout","text":["TRIAL 4; ACCURACY: 0.7878787878787878\n"]},{"output_type":"stream","name":"stderr","text":["loading weights file https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/e92c166da55aafd5132b2303f81e33adf5982014b93fbbd6a83b37a9d88dde7e.0eb6384a7113d96cb7ac12ab686050a3f5d328e6cc6f016bbba66af2ced30777\n","Some weights of the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 were not used when initializing XLMRobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["evaluator = Evaluator()\n","results = evaluator.runevaluation(n=5)"]},{"cell_type":"code","source":["results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JpYD0Vd_YEu","executionInfo":{"status":"ok","timestamp":1648630111182,"user_tz":-120,"elapsed":322,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}},"outputId":"52c2810d-124c-4589-e77e-35e7f2003a66"},"id":"2JpYD0Vd_YEu","execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.884848484848485, 'correct': 146, 'total': 165}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["evaluator.trials"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4xnJ867bAxkN","executionInfo":{"status":"ok","timestamp":1648630122494,"user_tz":-120,"elapsed":247,"user":{"displayName":"Aleksandr Barinov","userId":"07365249753635694017"}},"outputId":"549a05ce-68a4-4487-e455-100e43fc1b58"},"id":"4xnJ867bAxkN","execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'accuracy': 0.7878787878787878,\n","  'correct': 26,\n","  'groundtruth': array([ 7, 20, 12, 13, 10, 11,  0, 16, 17,  6, 13,  6, 17,  5,  3, 14, 22,\n","          7,  5, 16,  9, 12, 15, 21, 18,  0, 11, 13, 13,  8, 19, 18,  0]),\n","  'predicted': array([ 7, 20, 12, 13, 10, 11,  0, 16, 17,  6, 13,  6, 17,  4, 17, 14,  6,\n","          7,  4, 16, 11, 12, 15, 21, 18,  0, 11, 13, 13,  2, 19, 15,  0]),\n","  'total': 33},\n"," {'accuracy': 0.9696969696969697,\n","  'correct': 32,\n","  'groundtruth': array([ 9,  8,  3, 15, 11, 18,  9, 15,  9,  3, 19, 12, 20, 13, 18, 17,  6,\n","          2, 10, 19, 13, 19, 20, 21, 17,  1, 10, 15,  6,  9,  4, 13, 14]),\n","  'predicted': array([ 9,  8,  3, 15, 11, 18,  9, 15,  9, 17, 19, 12, 20, 13, 18, 17,  6,\n","          2, 10, 19, 13, 19, 20, 21, 17,  1, 10, 15,  6,  9,  4, 13, 14]),\n","  'total': 33},\n"," {'accuracy': 0.9393939393939394,\n","  'correct': 31,\n","  'groundtruth': array([20, 14,  6, 11, 13, 21,  2,  0, 20, 17, 21,  3, 17, 13,  8, 17, 13,\n","          6, 17,  9,  0, 19, 20, 18,  2, 13, 21,  9,  7,  6,  4, 18, 12]),\n","  'predicted': array([20, 14,  6, 11, 13, 21,  2,  0, 20, 17, 21,  3, 17, 13,  8, 17, 13,\n","          6, 17, 11,  0, 19, 20, 18,  2, 13, 21,  9,  7,  6,  4, 15, 12]),\n","  'total': 33},\n"," {'accuracy': 0.9393939393939394,\n","  'correct': 31,\n","  'groundtruth': array([ 6, 17, 11, 21, 15, 13,  3,  7, 13, 14,  1,  9,  0,  4, 21,  6,  0,\n","         12,  5, 13,  9, 11, 13,  7,  8, 17, 17, 17, 22,  0, 18, 13, 11]),\n","  'predicted': array([ 6, 17, 11, 21, 15, 13,  3,  7, 13, 14,  1,  9,  0,  4, 21,  6,  0,\n","         12,  4, 13,  9, 11, 13,  7,  8, 17, 17, 17,  6,  0, 18, 13, 11]),\n","  'total': 33},\n"," {'accuracy': 0.7878787878787878,\n","  'correct': 26,\n","  'groundtruth': array([ 7, 20, 12, 13, 10, 11,  0, 16, 17,  6, 13,  6, 17,  5,  3, 14, 22,\n","          7,  5, 16,  9, 12, 15, 21, 18,  0, 11, 13, 13,  8, 19, 18,  0]),\n","  'predicted': array([ 7, 20, 12, 13, 10, 11,  0, 16, 17,  6, 13,  6, 17,  4, 17, 14,  6,\n","          7,  4, 16, 11, 12, 15, 21, 18,  0, 11, 13, 13,  2, 19, 15,  0]),\n","  'total': 33}]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[""],"metadata":{"id":"hzp-zdh7A0R2"},"id":"hzp-zdh7A0R2","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"bertTuning.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a145b426a76e4d60afd3f47604e886c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a9069808874423eafabc8cf0fd01129","IPY_MODEL_052a41705c6848f89921aa1754ec2cd8","IPY_MODEL_6654138f106847b88af7e85998ac6212"],"layout":"IPY_MODEL_889ae363ce93458796bc404a56ac2055"}},"2a9069808874423eafabc8cf0fd01129":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f1089849a294cb5adfa27c14d6c6cd7","placeholder":"​","style":"IPY_MODEL_cffd692b7be448328cf21189d0bf0a92","value":"Downloading: 100%"}},"052a41705c6848f89921aa1754ec2cd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d02728df92b47f9877826945c724dfa","max":402,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f434ded0230542b3a763d8e9252633ed","value":402}},"6654138f106847b88af7e85998ac6212":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6090d4ff448e4e29aa810bf34f6c1678","placeholder":"​","style":"IPY_MODEL_6b1ce65d10b9475cbb76ee808465cca9","value":" 402/402 [00:00&lt;00:00, 3.22kB/s]"}},"889ae363ce93458796bc404a56ac2055":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f1089849a294cb5adfa27c14d6c6cd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cffd692b7be448328cf21189d0bf0a92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d02728df92b47f9877826945c724dfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f434ded0230542b3a763d8e9252633ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6090d4ff448e4e29aa810bf34f6c1678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1ce65d10b9475cbb76ee808465cca9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"650763fdff364a9ba49c8df6de752df4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad16009ecd744b4793bc3ad5c1c4070c","IPY_MODEL_39ee340158464be4abcdd88b8a3ae922","IPY_MODEL_2e2ae3b359e6479581108a8a831ea3b1"],"layout":"IPY_MODEL_799b59610d804f2db660a50bd56734c0"}},"ad16009ecd744b4793bc3ad5c1c4070c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be753dc64c0940968a19dfa50e973ece","placeholder":"​","style":"IPY_MODEL_b4a305349c1244c99f5cff87497c98c4","value":"Downloading: 100%"}},"39ee340158464be4abcdd88b8a3ae922":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0010d7b0b4d41caad8cd64b61917158","max":723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6769b62f9194f39ab18881a606aae52","value":723}},"2e2ae3b359e6479581108a8a831ea3b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bea93ddb82d4fe6a9b381677425451c","placeholder":"​","style":"IPY_MODEL_ebb9d50eb23047b180769c9f94bd9de4","value":" 723/723 [00:00&lt;00:00, 3.59kB/s]"}},"799b59610d804f2db660a50bd56734c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be753dc64c0940968a19dfa50e973ece":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4a305349c1244c99f5cff87497c98c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0010d7b0b4d41caad8cd64b61917158":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6769b62f9194f39ab18881a606aae52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bea93ddb82d4fe6a9b381677425451c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebb9d50eb23047b180769c9f94bd9de4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ffce6cbe1be4b7097ba044644909e21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77e5b8cfaf48476b8faf0e41ae5c8527","IPY_MODEL_d2860526b9c0406eb226ce9fc55affe6","IPY_MODEL_89e1dc3981504c6099654dae542cd8f1"],"layout":"IPY_MODEL_428a1b76726d487da85c1f5da374e230"}},"77e5b8cfaf48476b8faf0e41ae5c8527":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_800007ce6c0240428052863a69eb556c","placeholder":"​","style":"IPY_MODEL_2888f3e7fbb441079d526045fa9d90e4","value":"Downloading: 100%"}},"d2860526b9c0406eb226ce9fc55affe6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca1688a5b98d4e8e8edc5d2d4b5013b9","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4ca904cb3fa4963a148f02d7ac3f2b7","value":5069051}},"89e1dc3981504c6099654dae542cd8f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c83e3a307038482a9b9a519b3746637c","placeholder":"​","style":"IPY_MODEL_57431ebcd1c74e41acae04c81069ff4c","value":" 4.83M/4.83M [00:00&lt;00:00, 11.0MB/s]"}},"428a1b76726d487da85c1f5da374e230":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"800007ce6c0240428052863a69eb556c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2888f3e7fbb441079d526045fa9d90e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca1688a5b98d4e8e8edc5d2d4b5013b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4ca904cb3fa4963a148f02d7ac3f2b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c83e3a307038482a9b9a519b3746637c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57431ebcd1c74e41acae04c81069ff4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cd78538731d439887d49ecf62bf1983":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66b0e20fd801478dba87365e1cfc21ed","IPY_MODEL_8f038fa32d1749b2b09086992b95db0c","IPY_MODEL_ef0776a9fc864f42874b72af8262b92d"],"layout":"IPY_MODEL_3af79938ceed44a9a93dfbc15b537d39"}},"66b0e20fd801478dba87365e1cfc21ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2772b9d2524a41c4b5cba7e581bc7796","placeholder":"​","style":"IPY_MODEL_3e7652099b4d4039a418151202da6815","value":"Downloading: 100%"}},"8f038fa32d1749b2b09086992b95db0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce328c7084e4e709defaafb0f9f3602","max":9081518,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93822d4deae64a87925352c8a8e3d4f2","value":9081518}},"ef0776a9fc864f42874b72af8262b92d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de6bb4b1ed84496d953a23edf228759d","placeholder":"​","style":"IPY_MODEL_895bdca74aa04404b9e55538e9a987d0","value":" 8.66M/8.66M [00:00&lt;00:00, 11.6MB/s]"}},"3af79938ceed44a9a93dfbc15b537d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2772b9d2524a41c4b5cba7e581bc7796":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e7652099b4d4039a418151202da6815":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ce328c7084e4e709defaafb0f9f3602":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93822d4deae64a87925352c8a8e3d4f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de6bb4b1ed84496d953a23edf228759d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"895bdca74aa04404b9e55538e9a987d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1d50d684c4648ad84a6ee44ea7ae76f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbafaca1b32049efb814020f8b1f0e71","IPY_MODEL_f62d5157a315412ba7a3c2e163aa4a25","IPY_MODEL_89988da09b5a4db7a6a5762e3737d926"],"layout":"IPY_MODEL_7bbac0199ab84c198eabd88b1f38691a"}},"cbafaca1b32049efb814020f8b1f0e71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47cad33315c247a0a30e2814dce64b19","placeholder":"​","style":"IPY_MODEL_aa147de9b13c4afbb9d9f38edaf65503","value":"Downloading: 100%"}},"f62d5157a315412ba7a3c2e163aa4a25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51693424611146738c81ffc70c904ca0","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_595c24a6be1e47dab851407940d3d474","value":239}},"89988da09b5a4db7a6a5762e3737d926":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ed6e03f1ab94beba1cf1af576b4b47a","placeholder":"​","style":"IPY_MODEL_0d0b2a78e4704514ae387c5bcc49225b","value":" 239/239 [00:00&lt;00:00, 2.77kB/s]"}},"7bbac0199ab84c198eabd88b1f38691a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47cad33315c247a0a30e2814dce64b19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa147de9b13c4afbb9d9f38edaf65503":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51693424611146738c81ffc70c904ca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"595c24a6be1e47dab851407940d3d474":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ed6e03f1ab94beba1cf1af576b4b47a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d0b2a78e4704514ae387c5bcc49225b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"523ff370fef84e698441a8c3f75cea27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0815bd3c9f7a45838d4efd9cb63115f5","IPY_MODEL_f8505269a06444f7b6f01475b79a9527","IPY_MODEL_c50c7a4210f8451ca2dfb077745d424e"],"layout":"IPY_MODEL_8d690302588d4d73ad6d5d2e1db5d91c"}},"0815bd3c9f7a45838d4efd9cb63115f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c246930798614bce86a0bdd6406c2249","placeholder":"​","style":"IPY_MODEL_6de1063007034effa5640ac03f9f3e0f","value":"Downloading: 100%"}},"f8505269a06444f7b6f01475b79a9527":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1c19baba75b4c7fb2065f7f7b7329f6","max":1112253233,"min":0,"orientation":"horizontal","style":"IPY_MODEL_126d3e4e56b640a9ab43c8a0aa020bd1","value":1112253233}},"c50c7a4210f8451ca2dfb077745d424e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a677dce821544515a84844d9b868371e","placeholder":"​","style":"IPY_MODEL_3ab8819f5a0049dc9ff76b165734ef19","value":" 1.04G/1.04G [00:33&lt;00:00, 33.0MB/s]"}},"8d690302588d4d73ad6d5d2e1db5d91c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c246930798614bce86a0bdd6406c2249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6de1063007034effa5640ac03f9f3e0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1c19baba75b4c7fb2065f7f7b7329f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"126d3e4e56b640a9ab43c8a0aa020bd1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a677dce821544515a84844d9b868371e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ab8819f5a0049dc9ff76b165734ef19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}